---
title: "Follow-up epi study data manipulation"
author: "Zaw Myo Tun"
date: "15 August 2020"
output:
  html_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

In this tutorial, I share some functions and tricks to manipulate data for analysis of follow-up data using two by two contingency tables using some mock data. Follow-up data are typically collected from a cohort study or randomised controlled trials.  

You can download all files and data related to this tutorial from [my GitHub repository](https://github.com/zawmtun/seminarFollowUpData.git).  

Throughout the tutorial, function calls are denoted with a pair of parentheses at the end. For example, `sum()`. Packages are marked within a pair of curly brackets. For example, `{dplyr}`. The code can be found in gray boxes. The white boxes with each row preceded by `##` are the outputs of the code.   

**Pre-requisite:** I assume you are familiar with the basics of R and `{dplyr}` package. If you want to learn more about these, Chapter 3, 4, and 5 of the online book [R for Data Science](https://r4ds.had.co.nz/) by Garrett Grolemund and Hadley Wickham are helpful.     

We will use five packages in this tutorial. The code below will automatically load necessary packages. It will install the packages automatically if not found in the system.  

```{r packages}
pkgs <- c(
  "dplyr",
  "tidyr",
  "tibble",
  "epiR",
  "broom"
)

invisible(sapply(pkgs, function(.) if (!requireNamespace(.)) install.packages(.)))
invisible(sapply(pkgs, library, character.only = TRUE))
```

## 1. Objectives

1. Learn some useful functions helpful in filtering rows
2. Compute person-days at risk to the first positive result among participants with a negative result at baseline
3. Compare incidence rates using person-days from objective 2
4. Compute total person-days at risk for repeated positive results

## 2. Some useful functions

In this section, I will discuss a few functions that will come in handy in later part of the tutorial. You might feel that some of these are out of place for now. But it will make sense in the end. I promise!

### 2.1. Cumulative sum

The first function is `cumsum()`. It takes a numeric vector and returns its cumulative sum. This is straightforward.

```{r}
x <- c(1, 3, 6, 2, 2)
cumsum(x)
cumsum(c(0, 1, 0, 0, 1, 0, 1))
```

In the second example, did you notice how `cumsum()` preserves the sequence of changes between 0 and 1? We will come back to this in working with the example dataset.  

### 2.2. Select elements of a vector after the first `TRUE` inclusive

In `x` defined above, let's say we find the value `6` and select the elements that comes after it. That is, we will keep `6, 2, 2` and remove `1, 3`. For a single vector, we can do it using the position indexes.

```{r}
x
x[c(3:5)]
```

But this approach cannot scale -- for multiple vectors, one must manually look for the position of `6` and type in the index numbers. We need a different approach.   

`cumany()` comes to the rescue. It takes a logical vector and returns a logical vector which are `TRUE` for all elements after the first `TRUE`, inclusive, in the input. We can then use the output to select the elements from the vector.

```{r}
x
x == 6
cumany(x == 6)
x[cumany(x == 6)]

y <- 1:10
y == 6
cumany(y == 6)
y[cumany(y == 6)]
```

**Quiz:** What if we want the opposite? That is, we want to keep the elements before the first `TRUE`.

```{r}
# Hint: use "!" (NOT operator)

```

### 2.3. Compare a value with one before or after it

Sometimes, we want to know when a result changed from negative to positive or vice versa. Then, we compare a test result with the next one. How do we do it in R? One way is to use `lead()` and `lag()` from `{dplyr}`.  

`lag()` shifts the elements of a vector toward a higher index number by one position. Note that the empty element at the first index is filled with `NA` and the last of the shifted elements is truncated as `lag()` maintains the same length as the input vector.

```{r}
x <- c(0, 0, 1, 0, 0, 1)
x
lag(x)
```

By following `TRUE` in `x != lag(x)`, we know that values of x changed at the third, fourth, and sixth positions.

```{r}
x
x != lag(x)
```

`lead()` does the opposite of `lag()` -- it shifts the elements to a lower index number by one position.

```{r}
x
lead(x)
x != lead(x)
```

By default, `lead()` and `lag()` shift the elements by one position. We can change it by specifying a positive integer to `n =` argument. Also, the empty positions are filled with `NA` by default after shifting. We can change that in `default =` argument.  
 
```{r}
x
lag(x)
lag(x, n = 3)
lag(x, n = 3, default = 999)
```


```{r}
lead(x)
lead(x, n = 3)
lead(x, n = 3, default = 999)
```
 
## 3. Two example datasets

```{r readdata}
dat_ue <- readr::read_csv(here::here("data", "derived_data", "unequal_interval.csv"))
dat_e <- readr::read_csv(here::here("data", "derived_data", "equal_interval.csv"))
```

Both datasets have the same columns:

1. `id`: Study ID
2. `exposed`: Exposure status (0: Not exposed, 1: Exposed)
2. `d`: Follow-up date
3. `fu`: Follow-up instance
4. `result`: Test result (neg: Negative, pos: Positive)

The main difference between the two datasets is that dates are equally spaced by one day between two successive follow-up instances in one dataset but not in the other. Here are how the datasets look like.  

**Dataset 1: Unequal follow-up interval**

```{r}
head(dat_ue)
```

**Dataset 2: Equal follow-up interval**

```{r}
head(dat_e)
```

## 4. Unequal follow-up interval

Let's start with the first dataset (`dat_ue`). Our objective is to calculate incidence rate among participants with a negative result at baseline.  

### 4.1. Identify and remove the participants with a positive result at baseline

First, let's see how many participants were there in the dataset?

```{r}
# Number of unique IDs
dat_ue %>% 
  distinct(id) %>% 
  nrow()
```

Let's check how many participants (if any) had a positive result at baseline?

```{r}
dat_ue %>% 
  filter(fu == 1) %>% 
  count(result)
```

Four were positive at baseline. Since they are ineligible, we need to remove them. We can do it using two approaches. The first approach is to identify the ineligible participants first and then filter them out from the dataset using `%in%` operator.

```{r}
first_pos <- dat_ue %>% 
  filter(fu == 1 & result == "pos")

dat_ue_1 <- dat_ue %>%
  filter(!id %in% first_pos$id)
```

Let's check if it's done correctly.

```{r}
dat_ue_1 %>% 
  filter(fu == 1) %>% 
  count(result)
```

A second approach is to use `cumany()` and `group_by()`. `group_by(id)` groups the rows by `id`. Recall that `cumany()` looks for the first `TRUE` element starting from the first index. In `cumany()`, we identify participants with a positive result at the first follow-up instance and it marks `TRUE` in the remaining rows of such participants -- `FALSE` for other participants. Filtering rows with `TRUE` now would retain rows of participants with a positive result at the first follow-up instance. This is opposite of what we wanted. So, we need to flip the logical vector using NOT ("!") operator.   

```{r}
dat_ue_1 <- dat_ue %>%
  group_by(id) %>%
  filter(!cumany(fu == 1 & result == "pos")) %>%
  ungroup()
```

Let's check if it's done correctly.

```{r}
dat_ue_1 %>% 
  filter(fu == 1) %>% 
  count(result)
```

Both approaches may seem similar in terms of number of lines and code complexity. Personally, the second approach is more elegant in that subsequent operations can be chained together seamlessly without a need to create a separate vector to identify the ineligible participants.

### 4.2. Remove the rows after the positive result among cases

Now, let's count the positive cases among eligible participants.

```{r}
dat_ue_1 %>% 
  filter(result == "pos") %>% 
  distinct(id) %>% 
  nrow()
```

Among 11 eligible participants, 7 had at least one positive result throughout the follow-up period. Since we only want to compute incidence rate to the first positive result, we will identify the first positive result and remove the sequent rows.  

We first call `result == "pos"` which returns a logical vector. When we pass the vector to `cumsum()`, it was coerced into integer (`FALSE` to `0` and `TRUE` to `1`) before calculating cumulative sum. Rows with cumulative sum of one indicates both the first positive result and the negative results following it. We need to filter out the latter.

```{r}
dat_ue_2 <- dat_ue_1 %>%
  group_by(id) %>%
  mutate(flag = cumsum(result == "pos")) %>% 
  filter(flag <= 1) %>%
  filter(!(flag == 1 & result == "neg")) %>% 
  ungroup()

head(dat_ue_2)
```

Notice that the follow-up of participant 1 and 2 ends with the first positive result?  

### 4.3. Compute person-days at risk to the first positive result

To compute person-days at risk, we first need to determine follow-up start and end dates for each participant, using `min()` and `max()` over date column, respectively. The expression `result == "pos"` converts positive results into 1 which is repeated across the rows for each participant who had a positive result. At this point, values for exposure status, result, and start/end follow-up dates were the same across the rows for each participant. We remove duplicated rows using `distinct()`. Finally, we compute the person-days at risk by calculating the difference between start and end dates using `difftime()`. This creates `difftime` class column which cannot be used in summary computation. So, it is converted to a double.  

```{r}
dat_ue_3 <- dat_ue_2 %>%
  group_by(id) %>%
  mutate(start = min(d),
         end = max(d),
         result = max(result == "pos")) %>%
  ungroup() %>% 
  distinct(id, exposed, start, end, result) %>%
  mutate(duration = as.double(difftime(end, start, units = "days")))

dat_ue_3
```

At this point, the dataset only contains one row per participant.  

### 4.4. Compare incidence rates for exposed and unexposed

To compute incidence rates by exposure status, we use `epi.2by2()` from `{epiR}`. Take a look at the help file of the function (type `?epi.2by2` in the console). The first argument is `dat =` which accepts a `table` class object containing frequency numbers of a two by two contingency table. Here is how our table should look like.  

```{r}
dat <- as.table(matrix(c(5, 480, 2, 198), nrow = 2, byrow = TRUE))

class(dat)
dim(dat)
dat
```

The `table` has a dimension of 2 by 2; the outcome in columns; the exposure in rows. It was created by converting a matrix of four numbers as a `table`.  

Now, let's create a similar contingency table using `summarise()` from `{dplyr}`.  

```{r}
tab_df <- dat_ue_3 %>% 
  group_by(exposed) %>% 
  summarise(pos = sum(result),
            fu = sum(duration))

tab_df
```

This contingency table seems similar to the one above. But there are two problems. Let's check the class and dimensions of `tab_df`.

```{r}
dim(tab_df)
class(tab_df)
```

- **Problem 1:** It has 3 columns, instead of 2. Actually, the exposed column should be row names, instead of a column.
- **Problem 2:** Being part of Tidyverse, `summarise()` returns a dataframe, not a matrix/table.

We convert `exposed` column to row names using `column_to_rownames("exposed")`. That solves the first problem.  

It turns out that a dataframe can be converted to a matrix using `as.matrix()`. We can then convert this matrix to a table using `as.table()`. Viola! We get the table for `epi.2by2()`.

```{r}
tab <- tab_df %>% 
  column_to_rownames("exposed") %>%  # Convert a column to row names
  as.matrix() %>%  # Convert to matrix
  as.table() # Convert to a table

class(tab)
dim(tab)
tab
```

Then, we feed `tab` to `epi.2by2()`. We tell the function that this is a cohort study with follow-up time; 95% confidence level; 100 unit time (days in our example); outcome displayed as columns in the table.

```{r}
tab_epi <- epi.2by2(dat = tab,
                    method = "cohort.time",
                    units = 100,
                    outcome = "as.columns")

tab_epi
```

The output shows that incidence rate among exposed and unexposed were 1.04 and 1.01 per 100 person-days, respectively. Incidence rate ratio was 1.03 (95% CI: 0.17, 10.83). If you want to extract the point estimates and CIs, use `tidy()` from `{broom}`.


```{r}
tab_epi_tidy <- tidy(tab_epi)

tab_epi_tidy
```

I will close this section by demonstrating that the above steps can be chained up together easily.

```{r eval = FALSE}
tab_epi <- dat_ue_3 %>% 
  group_by(exposed) %>% 
  summarise(pos = sum(result),
            fu = sum(duration)) %>% 
  column_to_rownames("exposed") %>% 
  as.matrix() %>% 
  as.table() %>% 
  epi.2by2(method = "cohort.time",
           units = 100,
           outcome = "as.columns")
```

### 4.5. Calculate total person-days at risk for repeated positive results

Sometimes, we want to compute person-time for repeated positive results. Let's look at how it can be done.  

First, let's go back the original dataframe `dat_ue_1` where we have removed the ineligible participants. Here is what it looks like.

```{r echo = FALSE}
head(dat_ue_1, 16)
```

To get ready for computing person-days at risk, we need to turn `dat_ue_1` into this:

```{r echo = FALSE}
head(readRDS(here::here("data", "derived_data", "repeated_positive.rds")), 5)
```

We do it in a series of steps. These are a little bit tricky. Please pay attention to the changes in each step. You may need to view the dataset after executing each step.  

> A useful trick: One feature of RStudio I find useful is the ability to view the dataset by clicking on the dataframe name in a script file while pressing Control key on keyboard.   

- **Step 1:** Capture the sequence of results - Compare the current result with a previous one using `lag()`. This produce a logical vector which is then passed to `cumsum()`.

```{r}
s1 <- dat_ue_1 %>%
  group_by(id) %>%
  mutate(
    flag = lag(result, default = "missing"),
    flag = result != flag,
    seq = cumsum(flag)
  )

head(s1, 16)
```

- **Step 2:** Capture the earliest date for each sequence using `min()`. This notes the start date of the sequence as well as the end date of previous sequence.

```{r}
s2 <- s1 %>%
  group_by(id, seq) %>%
  mutate(d1 = min(d))

head(s2, 16)
```

- **Step 3:** The new date columns did not capture the follow-up end date of the participant. So, update `d1` using the last date from `fu`.

```{r}
s3 <- s2 %>%
  group_by(id) %>%
  mutate(d1 = if_else(fu == max(fu), d, d1))

head(s3, 16)
```


- **Step 4:** Now, `seq` and `d1` have the same values. Remove duplicated rows without removing all the columns. Note  `.keep_all = TRUE` in `distinct()` as it only keeps the columns specified in the call by default.

```{r}
# Reminder: Grouped by id
s4 <- s3 %>%
  distinct(id, seq, d1, .keep_all = TRUE)

head(s4, 10)
```

- **Step 5:** To compute person-time at risk, we only count the follow-up time when the participant's status is negative -- the time for the positive status should be removed. But all the results structured in a single column makes it difficult to filter the rows. We need two rows for results ("from" and "to") that tell us the direction of change in one row (negative to positive or vice versa). My strategy is to keep the existing result column as `from_res` and create a new column in which result column is pushed by one position for each participant using `lead()`.

```{r}
# Reminder: Grouped by id
s5 <- s4 %>%
  mutate(from_res = result,
         to_res = lead(result))

head(s5, 10)
```

- **Step 6:** Similarly, we need two columns for follow-up dates to capture the start and end dates of each result sequence. I use the strategy as step 5 and created a new column `d2` by pushing `d1` one row up. Then, follow-up time in result sequence is the different between `d2` and `d1`.

```{r}
# Grouped by id
s6 <- s5 %>%
  mutate(d2 = lead(d1),
         duration = as.double(difftime(d2, d1, units = "days")))

head(s6, 10)
```

- **Step 7:** The last row becomes redundant. So, I remove it using `slice(n())`. Then, only keep the necessary columns.

```{r}
# Reminder: Grouped by id
s7 <- s6 %>%
  slice(-n()) %>% 
  select(id, exposed, d1, d2, from_res, to_res, duration)

head(s7, 8)
```

- **Step 8:** Now, remove the rows in which results changed from positive to negative as their follow-up time will not be counted in the person-days at risk.

```{r}
# Reminder: Grouped by id
s8 <- s7 %>%
  filter(!(from_res == "pos" & to_res == "neg")) %>% 
  ungroup()

head(s8, 5)
```

It is easier to understand these steps if they are put together in a single chain. Here it is.

```{r steps, eval = FALSE}
s8 <- dat_ue_1 %>%
  group_by(id) %>%
  mutate(
    flag = lag(result, default = "missing"),
    flag = result != flag,
    seq = cumsum(flag)
  ) %>%
  group_by(id, seq) %>%
  mutate(d1 = min(d)) %>%
  group_by(id) %>%
  mutate(d1 = if_else(fu == max(fu), d, d1)) %>%
  distinct(id, seq, d1, .keep_all = TRUE) %>%
  mutate(from_res = result,
         to_res = lead(result)) %>%
  mutate(d2 = lead(d1),
         duration = as.double(difftime(d2, d1, units = "days"))) %>%
  slice(-n()) %>% 
  select(id, exposed, d1, d2, from_res, to_res, duration) %>%
  filter(!(from_res == "pos" & to_res == "neg")) %>% 
  ungroup()
```

### 4.6. Compare the incidence rates among exposed and unexposed participants with repeated positive results

**Quiz:** Compute incidence rate ratio and 95% CI using `s8`.

```{r}
#
```

## 5. Equal follow-up interval

The operations described above can also be used to manage datasets with equal follow-up interval. These data are more structured compared to those with unequal follow-up intervals. It means that follow-up dates and results for each follow-up instance can be easily pivoted to one column per instance and one row per participant. This makes filtering rows by dates and results much easier.

```{r}
dat_e_1 <- dat_e %>%
  pivot_wider(names_from = fu,
              values_from = c(d, result)) %>%

  # Remove patients with a positive result at the first follow-up
  filter(result_1 != "pos")

head(dat_e_1)
```

Note that it is one row per participant and that follow-up dates and results are in structured in columns with one columns for one follow-up instance.  

## 6. Conclusion

In this tutorial, we focused on data management and also cover some basic epidemiological analysis of time-to-event data. We learned some useful functions and used them to work through some examples using fake datasets. While these examples are useful, real life problems are obviously different and more complex. Chances are there is not a single function or a set of functions that are perfectly tailored for your needs. So, it is important to understand the logic and reasons behind each step so that you can use these basic building blocks to tackle more complex problems.

<br/>

**Session info (for reproducibility)**

```{r}
xfun::session_info()
```


